<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hank Is Lolo</title>
  
  
  <link href="http://hankislolo.com/atom.xml" rel="self"/>
  
  <link href="http://hankislolo.com/"/>
  <updated>2022-12-08T08:21:51.048Z</updated>
  <id>http://hankislolo.com/</id>
  
  <author>
    <name>Hank</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>VAD Research Logs</title>
    <link href="http://hankislolo.com/2022/12/07/VAD-Research-Logs/"/>
    <id>http://hankislolo.com/2022/12/07/VAD-Research-Logs/</id>
    <published>2022-12-07T09:33:16.000Z</published>
    <updated>2022-12-08T08:21:51.048Z</updated>
    
    <content type="html"><![CDATA[<h3 id="关于视频异常检测的核心问题">关于视频异常检测的核心问题</h3><hr><p>在过去的实验研究当中发现视频异常检测的具体实现有很多现实难以解决的问题。但是很多文章声称的具体方法却缺乏代码实现。本文对过去的文献加以介绍并总结，并添加一些实验中发现的问题，以启发后续的工作继续深入。</p><h4 id="mil-multiple-instance-learning-的一般方法">MIL (MultipleInstance Learning) 的一般方法</h4><p>VAD可以抽象为一个典型的多示例学习模型。由于感兴趣的正样本视频片段只占一个视频样本的一个小部分，因此可以将一个视频当作一个包(bag) ，每个包根据是否含有正片段示例 (instance)给定一个粗粒度的标签。对于一个典型的MIL问题，解决这个方法通常为<strong>迭代优化（alternativeoptimization)。</strong>也就是说，我们先假设已经知道了所有样本的标记，那么就可以通过某种监督学习的方法得到一个分类模型，通过这个模型我们可以对每个训练样本进行预测，然后更新它们的标记，我们又可以拿这一次新得到的标记重新训练分类模型了。所以整个优化过程分为两部分：监督学习，标记更新<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>。值得注意的是，在训练的过程中：</p><ol type="1"><li>第一点，训练监督学习的模型的时候，只从正样本包里挑选被预测的“最像正确”(也就是分类得分最高)的那一个，正样本包里面<strong>其他的</strong>样本，不管预测出来是正的还是负的<strong>都不要了</strong>。这是因为，其实多示例的问题也可以描述为，正样本包里面“最正确”的一个样本标记是正的，跟其他样本无关。所以，这种选择策略恰恰是符合问题定义的。<strong>（选最容易分类的，因为可能只有一个）</strong></li><li>第二点，如果负样本足够多的话，可以只挑选每个负样本包里面被预测“最像正确"的一个样本作为负样本进行训练，这样子的负样本也叫做hardsample或者most violatedsample。实践上来说，它们对于模型快速收敛是最有效的。<strong>（选最难分类的，因为所有样本都是负的，并且要使模型能够有区分度，这种观点可以参考支持向量机中的支持向量的观点）</strong><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li></ol><p>因此，对于一个经典的MIL问题，通常的算法实现过程分为如下步骤：</p><p>多示例学习：</p><p>输入：正包，负包</p><p>输出： 分类函数 <span class="math inline">\(f\)</span></p><p>将每个标记包中的样本初始化为<strong>包的标记</strong>，初始化集合<span class="math inline">\(U\)</span> 为空，将所有样本 (instance)加入样本集 <span class="math inline">\(U\)</span></p><p>重复下面的过程：</p><ol type="1"><li>取 <strong><span class="math inline">\(U\)</span>中所有样本</strong> <span class="math inline">\(s_i\)</span> 以及标记<span class="math inline">\(y\)</span> <em>训练</em> 得到一个分类函数<span class="math inline">\(f\)</span>（初始状态：所有包的样本都已经加入<span class="math inline">\(U\)</span>）</li><li>利用 <span class="math inline">\(f\)</span> <em>预测</em><strong>训练集的所有样本</strong>的标记 <span class="math inline">\(\hat{y}\)</span></li><li>清空 <span class="math inline">\(U\)</span></li><li>对于每个正标记包，选取 <span class="math inline">\(f\)</span>预测得分最高的样本 <span class="math inline">\(s_{p\_max}\)</span>加入集合 <span class="math inline">\(U\)</span>（选最容易分类的一个）</li><li>对于每个负标记包，选取 <span class="math inline">\(f\)</span>预测得分最高的样本 <span class="math inline">\(s_{n\_max}\)</span>加入集合 <span class="math inline">\(U\)</span>（选最难分类的一个或多个）</li></ol><p>直到 <span class="math inline">\(U\)</span> 中不包含正样本为止.</p><p>返回 <span class="math inline">\(f\)</span>.</p><p>伪代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Input: Negative_bags (N), Positive_bags (P)</span></span><br><span class="line"><span class="string">Output: Classifier f</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Args:</span></span><br><span class="line"><span class="string">iter_num: iterations of each training stage</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># Initialize labels for each of the instances of neg/pos bags</span></span><br><span class="line"><span class="keyword">for</span> instance, label <span class="keyword">in</span> N_bags:</span><br><span class="line">label = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> instance, label <span class="keyword">in</span> P_bags:</span><br><span class="line">label = <span class="number">1</span></span><br><span class="line">U.add(N, P)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative Optimization</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> condition:</span><br><span class="line">  <span class="comment"># Train model with current pseudo-labelled U</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iter_num):</span><br><span class="line">        model.train(U)  <span class="comment"># All positive instances are assigned as 1, else 0</span></span><br><span class="line">    U.clear()</span><br><span class="line">    N_bag, P_bag = [], []</span><br><span class="line">    <span class="keyword">for</span> bag <span class="keyword">in</span> N:</span><br><span class="line">        neg_instances = model.inference(bag)</span><br><span class="line">        N_bag.append(<span class="built_in">max</span>(neg_instances))  <span class="comment"># Pooling stage, add the most likely instance (representative) into training set U.</span></span><br><span class="line">    <span class="keyword">for</span> bag <span class="keyword">in</span> P:</span><br><span class="line">        pos_instances = model.inference(bag)</span><br><span class="line">        P_bag.append(<span class="built_in">max</span>(pos_instances))  <span class="comment"># Max-pooling</span></span><br><span class="line">    U.add(N_bag, P_bag)</span><br></pre></td></tr></table></figure><p>在迭代循环的过程中，1~3为迭代推理阶段，4~5为池化阶段 (Pooling) 。</p><p><del>池化操作会丢失大量的不必要数据。以以上方法为例，池化采用top-1则每一轮迭代的样本个数随包的大小指数级下降。在池化开始前，设每个包含有的样本数量大小<span class="math inline">\(N\)</span> 相同，<span class="math inline">\(U\)</span> 中的包个数为 <span class="math inline">\(m\)</span> ，则当前参与模型训练的样本 <span class="math inline">\(I\)</span> 的个数为 <span class="math inline">\(N\times m\)</span>. 在正负包平衡的状态下，池化操作使得 <span class="math inline">\(U\)</span> 包含的样本个数下降为 <span class="math inline">\(m\)</span>. 如果不能保证 <span class="math inline">\(m &gt;&gt; N\)</span>，</del></p><p>除了初始化将所有样本全部加入<span class="math inline">\(U\)</span>的情况，每一次迭代训练的正负样本数与池化操作得到的样本总量相同（在本例里，采用最大池化每个包挑选出一个样本作为代表）。</p><p>从以上一般MIL的求解范式来看，示例（instance,以上称为样本，为方便讨论以下将其称为“示例”）需要满足几个特征才适合于以上迭代优化的模式：</p><ul><li>Permutation-Invariance.交换示例的位置不会对打分造成影响（打分与位置无关）</li><li>所有的示例都应当具有相同的权重</li></ul><p>因此，对于视频异常检测任务而言，它可以使用MIL来进行建模，但是构建出的包质量会影响到预测函数<span class="math inline">\(f\)</span>的表现。具体而言，用MIL对VAD的建模天然采用一个完整的视频 (video)作为一个包 (bag) ，视频的部分片段 (clip) 作为示例 (instance)。直接用MIL对VAD任务建模存在一些重要问题：</p><ol type="1"><li>与标准的MIL不同的是，<strong>VAD的instances之间是具有时序关系的</strong>，而且随着采样时间长度的变化，示例的低维度特征也在变。</li><li>可能会对MIL模型造成致命影响的是对视频异常的定义：被定义为异常的片段通常是<strong>若干个连续示例</strong>（视频片段）的组合。<strong>其中的任何一个示例</strong>对上述分类器<span class="math inline">\(f\)</span>而言都<strong>不会</strong>被训练为正标记。如果增加示例长度的采样长度，一方面会使数据集的示例量总体减少，损失细节信息；另一方面会使得模型在正向推理的过程中只能对更粗粒度的视频片段作出预测。</li></ol><p>因此，对VAD进行简单的MIL建模是存在重要缺陷的。</p><h4 id="vad-不那么mil的mil">VAD: 不那么MIL的MIL</h4><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr><ol><li id="fn1"><p>https://zhuanlan.zhihu.com/p/377220948<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2"><p>https://zhuanlan.zhihu.com/p/377220948<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;关于视频异常检测的核心问题&quot;&gt;关于视频异常检测的核心问题&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;在过去的实验研究当中发现视频异常检测的具体实现有很多现实难以解决的问题。但是很多文章声称的具体方法却缺乏代码实现。本文对过去的文献加以介绍并总结，并添加一些实验中发现的问题，以启发</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Say Hi to Roxy</title>
    <link href="http://hankislolo.com/2022/12/07/Say-Hi-to-Roxy/"/>
    <id>http://hankislolo.com/2022/12/07/Say-Hi-to-Roxy/</id>
    <published>2022-12-07T05:36:06.000Z</published>
    <updated>2022-12-07T15:52:34.422Z</updated>
    
    <content type="html"><![CDATA[<p>Hello, Roxy! This is 4 U! Hank is Lolo.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hello, Roxy! This is 4 U! Hank is Lolo.&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
</feed>
