<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hank Is Lolo</title>
  
  
  <link href="http://hankislolo.com/atom.xml" rel="self"/>
  
  <link href="http://hankislolo.com/"/>
  <updated>2022-12-12T04:18:43.701Z</updated>
  <id>http://hankislolo.com/</id>
  
  <author>
    <name>Hank</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Experiment Chat with Bot</title>
    <link href="http://hankislolo.com/2022/12/12/Experiment-Chat-with-Bot/"/>
    <id>http://hankislolo.com/2022/12/12/Experiment-Chat-with-Bot/</id>
    <published>2022-12-12T03:40:03.000Z</published>
    <updated>2022-12-12T04:18:43.701Z</updated>
    
    <content type="html"><![CDATA[<p>以下是实验过程中发现问题并与机器人<a href="https://chat.openai.com/chat">ChatBot</a>讨论的过程。记录如下以便思路的开拓：</p><h4 id="关于预测信号异常为0和1的问题">关于预测信号异常为0和1的问题</h4><blockquote><p>Q: When I use MIL to tackle video anomaly detection task, I got aweird outcome that all scores of all positive bags and negative bags areclosed to 0. But I expect that positive instances are likely to be 1. Iuse hinge loss as my loss function, and aggregation function is maximum.Can you help analyse it?</p></blockquote><p>A: It sounds like you're running into a problem with your lossfunction and aggregation function. In the case of MIL, the idea is totake a collection of instances (called a bag) and determine whether thebag contains any positive instances. To do this, you typically want touse a loss function that encourages the model to output higher valuesfor bags that contain positive instances and lower values for bags thatdo not contain positive instances.</p><p>The maximum aggregation function, on the other hand, takes themaximum value of all the instances in the bag and uses that as the bag'sfinal output. This can lead to problems when the bag contains bothpositive and negative instances, because the maximum function willalways output the maximum value, regardless of whether it comes from apositive or negative instance.</p><p>To solve this problem, you may want to try using a different lossfunction and aggregation function. For example, you could try using thesum or mean aggregation function, which will take into account all theinstances in the bag, rather than just the maximum value. You could alsotry using a loss function that is better suited to MIL tasks, such asthe binary cross-entropy loss.</p><p>Overall, the specific loss function and aggregation function you usewill depend on the details of your task and the requirements of yourmodel. It's worth experimenting with different combinations to find theone that works best for your problem.</p><h4 id="损失函数的选择和不下降的问题">损失函数的选择和不下降的问题</h4><blockquote><p>Q: I tried BCE loss. However the loss would not decrease. I have noidea what happened.</p></blockquote><p>A: If the loss is not decreasing when using the binary cross-entropy(BCE) loss function, it could be due to a variety of reasons. Somepossible causes include:</p><ul><li>The model may <strong>not be learning at all</strong>, which couldbe due to a poor choice of model architecture, a lack of training data,or incorrect model hyperparameters.</li><li>The model may be <strong>overfitting</strong> to the training data,which can cause the loss to stop decreasing even if the model is stilllearning. This can happen if the model is too complex for the amount oftraining data available, or if the training data is not representativeof the true distribution of the data.</li><li>The <strong>learning rate may be too high</strong>, which can causethe model to oscillate or diverge, instead of converging on a minimumloss value.</li></ul><p>To diagnose the problem, it may be helpful to visualize the lossduring training using a tool like TensorBoard. This can give you abetter idea of what's happening during training, and can help youidentify potential issues with the model or the training process.</p><p>If the loss is not decreasing, you may want to try adjusting the<strong>model architecture</strong>, <strong>the training data</strong>,<strong>the learning rate</strong>, or <strong>otherhyperparameters</strong> to see if that helps the model learn moreeffectively. It's also important to make sure that the training data isrepresentative of the true distribution of the data, as this can affectthe model's ability to generalize to new, unseen examples.</p><h4 id="如何向多示例学习加入先验知识">如何向多示例学习加入先验知识</h4><blockquote><p>Q: I want to add some prior information to the MIL training. Theinput samples are extracted features, and I can compute saliency of eachinstance in a bag. How can I accomplish it?</p></blockquote><p>A: To incorporate prior information into a multiple instance learning(MIL) model, you can try <strong>using a regularization term</strong> inthe loss function that encourages the model to use the prior informationwhen making predictions.</p><p>For example, suppose you have a bag of instances, and you havecomputed the saliency of each instance in the bag. You can then use thissaliency information as a regularization term in the loss function, sothat the model is encouraged to give more weight to instances withhigher saliency when making predictions for the bag.</p><p>To do this, you would need to define a regularization term that takesinto account the saliency of each instance in the bag. For example, youcould define the regularization term as the sum of the saliency valuesof all the instances in the bag, multiplied by a regularization strengthhyperparameter. This would encourage the model to give more weight toinstances with higher saliency, and would help the model use the priorinformation more effectively when making predictions.</p><p>In general, the specific form of the regularization term will dependon the details of your task and the requirements of your model. It'sworth experimenting with different regularization terms to see whatworks best for your problem.</p><h4 id="数据可视化的技巧">数据可视化的技巧</h4><h5 id="关于可视化工具">关于可视化工具</h5><blockquote><p>Q: when I tried to visualize my training data information, I found itextremely complicated to redefinate functions via matplotlib. Any bettermethods to visualize my tensor data which are more handy (for me, eventensorboard is kind of complicated)?</p></blockquote><p>A: If you find it difficult to visualize your data using tools likematplotlib or TensorBoard, there are other options you can try. Forexample, you can use a tool like TensorFlow Playground(https://playground.tensorflow.org/) to interactively visualize andexplore your data. This can be a useful way to get a betterunderstanding of your data, and can help you identify patterns andtrends that may not be immediately obvious.</p><p>Another option is to use a tool like Embedding Projector(https://projector.tensorflow.org/) to visualize high-dimensional datain a lower-dimensional space. This can be particularly useful forvisualizing data that has many features or dimensions, as it can helpyou understand the relationships between different features and identifyclusters or patterns in the data.</p><p>Overall, there are many different tools and techniques you can use tovisualize your data, and the best one for you will depend on thespecific details of your data and the goals of your visualization. It'sworth experimenting with different tools and techniques to see whatworks best for your problem.</p><p>注：ChatBox所说EmbeddingProjector是一个离线的可视化工具，需要将训练过程中的embedding转化成TSV文件并上传到上述网站中，获取可视化的图像。</p><p>要将实验数据可视化，首先确定要研究什么统计量，最小单元是什么。在训练过程中的损失变化还是使用tensorboard比较合适，但是其他的变量可以用可视化工具进行高级显示。</p><h5 id="关于将训练代码和可视化分离代码分离的技巧">关于将训练代码和可视化分离代码分离的技巧</h5><blockquote><p>Q: In my deep learning codings, I want to visualize tensors in mytraining procedure. Anyway I don't want to mixup the visualizationcoding and training coding which makes my code less maintainable. Inwhat way can I reconstruct my code to better satisfy my task?</p></blockquote><p>A: One way to separate the visualization code from the training codeis to use a callback function. In deep learning, a callback is afunction that is executed at the end of each training epoch. This allowsyou to separate the code for visualizing the training process from thecode that actually trains the model.</p><p>Here is an example of how you might use a callback function tovisualize the training process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a callback function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_training</span>(<span class="params">model</span>):</span><br><span class="line">  <span class="comment"># Generate some data to use for visualizing the training process</span></span><br><span class="line">  data = generate_data()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Use the model to make predictions on the data</span></span><br><span class="line">  predictions = model.predict(data)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Visualize the predictions</span></span><br><span class="line">  visualize(predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the callback function as a callback during training</span></span><br><span class="line">model.fit(data, labels, epochs=<span class="number">10</span>, callbacks=[visualize_training])</span><br></pre></td></tr></table></figure><p>This way, the code for visualizing the training process is containedwithin the <code>visualize_training()</code> function, and can be easilyreused or modified without affecting the rest of the training code.</p><p>Here is an example of how you might implement a callback function inPyTorch:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the necessary modules</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a callback function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_training</span>(<span class="params">model, data_loader</span>):</span><br><span class="line">  <span class="comment"># Loop through the data in the data loader</span></span><br><span class="line">  <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> data_loader:</span><br><span class="line">    <span class="comment"># Use the model to make predictions on the inputs</span></span><br><span class="line">    predictions = model(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Visualize the predictions</span></span><br><span class="line">    visualize(predictions, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a data loader</span></span><br><span class="line">data_loader = DataLoader(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the callback function as a callback during training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">  <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> data_loader:</span><br><span class="line">    <span class="comment"># Perform a training step</span></span><br><span class="line">    output = model(inputs)</span><br><span class="line">    loss = criterion(output, labels)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Use the callback after each epoch</span></span><br><span class="line">  visualize_training(model, data_loader)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注：实际上这里的Callback就是在每一轮训练的结束阶段调用一个外部函数将需要可视化的数据向里边传。机器人给的代码可以看到，传入的数据是一个dataloader，也就意味着所有的数据都要通过dataloader重新加载一遍。它的好处在于可以在外部函数根据需要定制数据的输出，而不必在训练函数内部反复调试代码。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;以下是实验过程中发现问题并与机器人&lt;a href=&quot;https://chat.openai.com/chat&quot;&gt;ChatBot&lt;/a&gt;讨论的过程。记录如下以便思路的开拓：&lt;/p&gt;
&lt;h4 id=&quot;关于预测信号异常为0和1的问题&quot;&gt;关于预测信号异常为0和1的问题&lt;/h4&gt;
&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>VAD Research Review</title>
    <link href="http://hankislolo.com/2022/12/08/VAD-Research-Review/"/>
    <id>http://hankislolo.com/2022/12/08/VAD-Research-Review/</id>
    <published>2022-12-08T09:36:46.000Z</published>
    <updated>2022-12-09T02:54:05.369Z</updated>
    
    <content type="html"><![CDATA[<p>本文档记录VAD的各类解决方案，并对其方法进行归纳。</p><table style="width:100%;"><colgroup><col style="width: 16%"><col style="width: 10%"><col style="width: 16%"><col style="width: 15%"><col style="width: 4%"><col style="width: 16%"><col style="width: 4%"><col style="width: 16%"></colgroup><thead><tr class="header"><th>Title</th><th style="text-align: center;">Keyword</th><th>Idea</th><th style="text-align: center;">Pooling</th><th style="text-align: left;">Advantages</th><th>Drawbacks</th><th>Note</th><th>Comment</th></tr></thead><tbody><tr class="odd"><td><a href="https://link.springer.com/article/10.1007/s11227-021-04190-9">Attention‐basedframework for weakly supervised video anomaly detection</a></td><td style="text-align: center;">GAAM, DAAM</td><td><strong>双任务</strong>结合（异常注意力+视频特征重构）</td><td style="text-align: center;"></td><td style="text-align: left;"></td><td></td><td>文章未开源，无代码</td><td></td></tr><tr class="even"><td><a href="https://www.aaai.org/AAAI22Papers/AAAI-6637.LiS.pdf">Self-TrainingMulti-Sequence Learning with Transformer for Weakly Supervised VideoAnomaly Detection</a></td><td style="text-align: center;">MSL</td><td>将Instance改为Sequence，在迭代优化的过程中逐渐减小Sequence的大小，使预测信号在时间域上逐渐分离。使用CTE模块，引入的Conv可以保留局部上下文特征</td><td style="text-align: center;"><strong>Sequence-Pooling(Dynamic)</strong></td><td style="text-align: left;">自适应的渐变优化</td><td>尚未复现。初步实验发现CTE在全局的训练过程中会让所有的特征分数下降为0或者1（这种现象似乎是正常的？）过程实现很繁琐</td><td>无代码</td><td></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/abstract/document/9774889">WeaklySupervised Video Anomaly Detection via Transformer-Enabled TemporalRelation Learning</a></td><td style="text-align: center;">Transformer, Temporal RelationLearning</td><td>在I3D特征后引入上下文的Transformer自注意力模块，将上<strong>下文信息融合</strong>到每一个示例的特征（高阶）当中</td><td style="text-align: center;">mean</td><td style="text-align: left;"></td><td></td><td>未开源</td><td>上下文注意力肯定是合理的，但是它比Conv模块提取局部信息快了多少？</td></tr><tr class="even"><td><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640328.pdf">Self-TrainingMulti-Sequence Learning with Transformer for Weakly Supervised VideoAnomaly Detection</a></td><td style="text-align: center;">PSR (Patch Spatio Relation)</td><td></td><td style="text-align: center;">使用了RTFM的池化方法，选择了top-kmagnitude （特征二范数）</td><td style="text-align: left;"></td><td></td><td></td><td></td></tr><tr class="odd"><td></td><td style="text-align: center;"></td><td></td><td style="text-align: center;"></td><td style="text-align: left;"></td><td></td><td></td><td></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文档记录VAD的各类解决方案，并对其方法进行归纳。&lt;/p&gt;
&lt;table style=&quot;width:100%;&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 16%&quot;&gt;
&lt;col style=&quot;width: 10%&quot;&gt;
&lt;col style=&quot;width</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>VAD Research Logs</title>
    <link href="http://hankislolo.com/2022/12/07/VAD-Research-Logs/"/>
    <id>http://hankislolo.com/2022/12/07/VAD-Research-Logs/</id>
    <published>2022-12-07T09:33:16.000Z</published>
    <updated>2022-12-09T09:39:40.244Z</updated>
    
    <content type="html"><![CDATA[<h3 id="关于视频异常检测的核心问题">关于视频异常检测的核心问题</h3><hr><p>在过去的实验研究当中发现视频异常检测的具体实现有很多现实难以解决的问题。但是很多文章声称的具体方法却缺乏代码实现。本文对过去的文献加以介绍并总结，并添加一些实验中发现的问题，以启发后续的工作继续深入。</p><h4 id="mil-multiple-instance-learning-的一般方法">MIL (MultipleInstance Learning) 的一般方法</h4><p>对于一个典型的MIL问题，解决这个方法通常为<strong>迭代优化（alternativeoptimization)。</strong>也就是说，我们先假设已经知道了所有样本的标记，那么就可以通过某种监督学习的方法得到一个分类模型，通过这个模型我们可以对每个训练样本进行预测，然后更新它们的标记，我们又可以拿这一次新得到的标记重新训练分类模型了。所以整个优化过程分为两部分：监督学习，标记更新<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>。值得注意的是，在训练的过程中：</p><ol type="1"><li>第一点，训练监督学习的模型的时候，只从正样本包里挑选被预测的“最像正确”(也就是分类得分最高)的那一个，正样本包里面<strong>其他的</strong>样本，不管预测出来是正的还是负的<strong>都不要了</strong>。这是因为，其实多示例的问题也可以描述为，正样本包里面“最正确”的一个样本标记是正的，跟其他样本无关。所以，这种选择策略恰恰是符合问题定义的。<strong>（选最容易分类的，因为可能只有一个）</strong></li><li>第二点，如果负样本足够多的话，可以只挑选每个负样本包里面被预测“最像正确"的一个样本作为负样本进行训练，这样子的负样本也叫做hardsample或者most violatedsample。实践上来说，它们对于模型快速收敛是最有效的。<strong>（选最难分类的，因为所有样本都是负的，并且要使模型能够有区分度，这种观点可以参考支持向量机中的支持向量的观点）</strong><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li></ol><p>因此，对于一个经典的MIL问题，通常的算法实现过程分为如下步骤：</p><p>多示例学习：</p><p>输入：正包，负包</p><p>输出： 分类函数 <span class="math inline">\(f\)</span></p><p>将每个标记包中的样本初始化为<strong>包的标记</strong>，初始化集合<span class="math inline">\(U\)</span> 为空，将所有样本 (instance)加入样本集 <span class="math inline">\(U\)</span></p><p>重复下面的过程：</p><ol type="1"><li>取 <strong><span class="math inline">\(U\)</span>中所有样本</strong> <span class="math inline">\(s_i\)</span> 以及标记<span class="math inline">\(y\)</span> <em>训练</em> 得到一个分类函数<span class="math inline">\(f\)</span>（初始状态：所有包的样本都已经加入<span class="math inline">\(U\)</span>）</li><li>利用 <span class="math inline">\(f\)</span> <em>预测</em><strong>训练集的所有样本</strong>的标记 <span class="math inline">\(\hat{y}\)</span></li><li>清空 <span class="math inline">\(U\)</span></li><li>对于每个正标记包，选取 <span class="math inline">\(f\)</span>预测得分最高的样本 <span class="math inline">\(s_{p\_max}\)</span>加入集合 <span class="math inline">\(U\)</span>（选最容易分类的一个）</li><li>对于每个负标记包，选取 <span class="math inline">\(f\)</span>预测得分最高的样本 <span class="math inline">\(s_{n\_max}\)</span>加入集合 <span class="math inline">\(U\)</span>（选最难分类的一个或多个）</li></ol><p>直到 <span class="math inline">\(U\)</span> 中不包含正样本为止.</p><p>返回 <span class="math inline">\(f\)</span>.</p><p>伪代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Input: Negative_bags (N), Positive_bags (P)</span></span><br><span class="line"><span class="string">Output: Classifier f</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Args:</span></span><br><span class="line"><span class="string">iter_num: iterations of each training stage</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># Initialize labels for each of the instances of neg/pos bags</span></span><br><span class="line"><span class="keyword">for</span> instance, label <span class="keyword">in</span> N_bags:</span><br><span class="line">label = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> instance, label <span class="keyword">in</span> P_bags:</span><br><span class="line">label = <span class="number">1</span></span><br><span class="line">U.add(N, P)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative Optimization</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> condition:</span><br><span class="line">  <span class="comment"># Train model with current pseudo-labelled U</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iter_num):</span><br><span class="line">        model.train(U)  <span class="comment"># All positive instances are assigned as 1, else 0</span></span><br><span class="line">    U.clear()</span><br><span class="line">    N_bag, P_bag = [], []</span><br><span class="line">    <span class="keyword">for</span> bag <span class="keyword">in</span> N:</span><br><span class="line">        neg_instances = model.inference(bag)</span><br><span class="line">        N_bag.append(<span class="built_in">max</span>(neg_instances))  <span class="comment"># Pooling stage, add the most likely instance (representative) into training set U.</span></span><br><span class="line">    <span class="keyword">for</span> bag <span class="keyword">in</span> P:</span><br><span class="line">        pos_instances = model.inference(bag)</span><br><span class="line">        P_bag.append(<span class="built_in">max</span>(pos_instances))  <span class="comment"># Max-pooling</span></span><br><span class="line">    U.add(N_bag, P_bag)</span><br></pre></td></tr></table></figure><p>在迭代循环的过程中，1~3为迭代推理阶段，4~5为池化阶段 (Pooling) 。</p><p><del>池化操作会丢失大量的不必要数据。以以上方法为例，池化采用top-1则每一轮迭代的样本个数随包的大小指数级下降。在池化开始前，设每个包含有的样本数量大小<span class="math inline">\(N\)</span> 相同，<span class="math inline">\(U\)</span> 中的包个数为 <span class="math inline">\(m\)</span> ，则当前参与模型训练的样本 <span class="math inline">\(I\)</span> 的个数为 <span class="math inline">\(N\times m\)</span>. 在正负包平衡的状态下，池化操作使得 <span class="math inline">\(U\)</span> 包含的样本个数下降为 <span class="math inline">\(m\)</span>. 如果不能保证 <span class="math inline">\(m &gt;&gt; N\)</span>，</del></p><p>除了初始化将所有样本全部加入<span class="math inline">\(U\)</span>的情况，每一次迭代训练的正负样本数与池化操作得到的样本总量相同（在本例里，采用最大池化每个包挑选出一个样本作为代表）。每一次迭代完的模型在推理过程中会对正负包里面的所有样本重新打上0/1标签。</p><p>从以上MIL的一般求解范式来看，MIL具有以下特点：</p><ul><li>示例（instance,以上称为样本，为方便讨论以下将其称为“示例”）具有Permutation-Invariance特性，即交换示例的位置不会对打分造成影响（打分与位置无关）</li><li>模型的训练过程中“包”的概念被解除，预测模型的输入为单个的示例（更加细的粒度）</li><li><input type="checkbox" disabled>目标预测函数最好对数据分布具有一定程度的先验知识（第一轮选出来的示例代表非常重要，如果选择错误会导致后续的训练也产生错误）</li></ul><hr><p>chatbot给的MIL伪代码：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">function MIL(instance_groups, labels):</span><br><span class="line"></span><br><span class="line">  for each group, label in zip(instance_groups, labels):</span><br><span class="line">    for instance in group:</span><br><span class="line">      predict_label = classify(instance)</span><br><span class="line">      if predict_label == label:</span><br><span class="line">        # instance is correctly classified</span><br><span class="line">        update_weights(instance, predict_label)</span><br><span class="line">      else:</span><br><span class="line">        # instance is misclassified</span><br><span class="line">        update_weights(instance, label)</span><br><span class="line"></span><br><span class="line">  return classifier</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>该伪代码实现了MIL算法的基本思路。它首先遍历所有的实例组和标签，并对每个实例进行分类。如果预测的标签与真实标签相同，则更新分类器的权重，否则更新实例的权重。最后，它返回训练后的分类器。</p><hr><p>假设你使用的是MLP作为MIL模型，每一轮训练输入的是一对正负实例组，并使用最大值函数作为聚合函数，然后用sigmoid函数将聚合结果映射到[0,1]之间。那么，你可以使用HingeLoss作为目标损失函数来进行反向传播，输入目标损失函数的值就是sigmoid函数的输出。</p><p>具体来说，假设你有一个正样本<span class="math inline">\(x_1\)</span>和一个负样本<span class="math inline">\(x_2\)</span>，并将它们合并为一个输入<span class="math inline">\(x\)</span>，那么你可以使用MLP来计算它们的聚合值<span class="math inline">\(y\)</span>，具体来说：</p><p><span class="math inline">\(y = \max({f_{MLP}(x_1),f_{MLP}(x_2)})\)</span></p><p>其中<span class="math inline">\(f_{MLP}\)</span>表示MLP模型的输出。然后，你可以使用sigmoid函数来映射<span class="math inline">\(y\)</span>到[0,1]之间，即：</p><p><span class="math inline">\(z = \sigma(y)\)</span></p><p>其中<span class="math inline">\(\sigma\)</span>表示sigmoid函数。最后，你可以使用HingeLoss来计算模型的损失值，具体来说：</p><p><span class="math inline">\(L = \max({0, 1 - z})\)</span></p><p>这里的<span class="math inline">\(L\)</span>表示模型的损失值，<span class="math inline">\(1\)</span>表示正样本的标签，<span class="math inline">\(z\)</span>表示sigmoid函数的输出。</p><h5 id="关于损失函数的选择">关于损失函数的选择</h5><p>指数损失函数是一种用于二分类问题的目标损失函数，它可以用来衡量预测值与真实值之间的差距。指数损失函数的公式如下：</p><p><span class="math display">\[L(y, \hat{y}) = \begin{cases} - \log(\hat{y_i}) &amp; \text{if } y_i = 1\ - \log(1 - \hat{y_i})  \text{if } y_i = 0 \end{cases}\]</span></p><p>在进行基于MIL的视频异常检测时，需要选择一个合适的聚合函数来对多个数据集的预测结果进行聚合。常用的聚合函数包括平均值聚合函数、加权平均值聚合函数和最大值聚合函数等。</p><p>平均值聚合函数是一种常用的聚合函数，它将多个数据集的预测结果求平均值。平均值聚合函数的公式如下：</p><p><span class="math inline">\(f(y_1, y_2, \dots, y_n) = \frac{1}{n}\sum_{i=1}^{n} y_i\)</span></p><p>其中，<span class="math inline">\(y_1, y_2, \dots, y_n\)</span>是多个数据集的预测结果，<span class="math inline">\(n\)</span>是数据集的数量。</p><p>加权平均值聚合函数是另一种常用的聚合函数，它将多个数据集的预测结果按照一定的权重进行加权平均。加权平均值聚合函数的公式如下：</p><p><span class="math inline">\(f(y_1, y_2, \dots, y_n) =\frac{\sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i}\)</span></p><p>其中，<span class="math inline">\(y_1, y_2, \dots, y_n\)</span>是多个数据集的预测结果，<span class="math inline">\(w_1, w_2, \dots,w_n\)</span> 是数据集的权重，<span class="math inline">\(n\)</span>是数据集的数量。</p><p>取最大值聚合函数的公式如下：</p><p><span class="math inline">\(f(y_1, y_2, \dots, y_n) = \max(y_1, y_2,\dots, y_n)\)</span></p><p>其中，<span class="math inline">\(y_1, y_2, \dots, y_n\)</span>是多个数据集的预测结果，<span class="math inline">\(n\)</span>是数据集的数量。</p><p>需要注意的是，在使用HingeLoss作为目标损失函数时，还需要考虑其它因素，比如样本权重、正负样本比例等。</p><p>具体来说，对于一个样本<span class="math inline">\(x\)</span>，如果它是正样本，那么其权重<span class="math inline">\(w_x\)</span>可以设为1；如果它是负样本，那么其权重<span class="math inline">\(w_x\)</span>可以设为正负样本比例的倒数，即<span class="math inline">\(w_x = \frac{1}{p}\)</span>，其中<span class="math inline">\(p\)</span>表示正负样本比例。这样可以确保正负样本在训练过程中贡献的损失值相等。</p><p>同时，需要注意的是，使用HingeLoss作为目标损失函数时，可能会出现梯度消失或爆炸的问题。因此，在使用HingeLoss时，需要注意模型的参数初始化方式，并选择合适的优化算法，以避免梯度消失或爆炸的问题。</p><p>总之，在使用HingeLoss作为目标损失函数时，需要注意样本权重、正负样本比例以及梯度消失或爆炸的问题。</p><p>除了HingeLoss之外，还可以使用其它的损失函数作为目标损失函数，例如交叉熵损失（Cross-EntropyLoss）、平方损失（Square Loss）、指数损失（Exponential Loss）等。</p><p>具体来说，对于一个样本<span class="math inline">\(x\)</span>，如果它是正样本，那么其权重<span class="math inline">\(w_x\)</span>可以设为1；如果它是负样本，那么其权重<span class="math inline">\(w_x\)</span>可以设为正负样本比例的倒数，即<span class="math inline">\(w_x = \frac{1}{p}\)</span>，其中<span class="math inline">\(p\)</span>表示正负样本比例。然后，可以使用如下方法来计算损失值：</p><ul><li>交叉熵损失：<span class="math inline">\(L_x = -w_x\log{z_x}\)</span>，其中<span class="math inline">\(z_x\)</span>表示模型对样本<span class="math inline">\(x\)</span>的预测值。</li><li>平方损失：<span class="math inline">\(L_x = w_x (1 -z_x)^2\)</span>，其中<span class="math inline">\(z_x\)</span>表示模型对样本<span class="math inline">\(x\)</span>的预测值。</li><li>指数损失：<span class="math inline">\(L_x = w_x \exp{(1 -z_x)}\)</span>，其中<span class="math inline">\(z_x\)</span>表示模型对样本<span class="math inline">\(x\)</span>的预测值。</li></ul><p>上面的损失函数是常用的目标损失函数，但并不是唯一的选择。根据实际情况，可以选择合适的目标损失函数，以便更好地拟合模型。</p><p>另外，需要注意的是，在使用不同的目标损失函数时，可能会出现梯度消失或爆炸的问题。因此，在使用损失函数时，需要注意模型的参数初始化方式，并选择合适的优化算法，以避免梯度消失或爆炸的问题。</p><p>在采用高斯分布对实例组进行预处理时，预处理阶段应当发生在实例送入 MIL模型之前。这是因为，预处理阶段的目的是对实例组进行标准化，以便更好地送入MIL 模型进行处理。</p><p>采用高斯分布进行预处理的流程可以简述如下：首先，计算实例组的均值和标准差；然后，使用均值和标准差对实例组进行标准化，即将实例组的每个实例减去均值，再除以标准差；最后，将标准化后的实例组送入MIL 模型进行处理。</p><p>另外，在实例组送入 MIL 模型之后，你也可以使用高斯分布对 MIL模型的激活值进行预处理。这样做的目的是让激活值更好地拟合高斯分布，从而更好地适应MIL 模型的处理逻辑。但是，这种方法的精度可能比送入 MIL模型之前的预处理方法低一些，因为在 MIL模型的处理过程中可能会丢失一些信息。</p><h4 id="vad-不那么mil的mil">VAD: 不那么MIL的MIL</h4><p>VAD可以抽象为一个典型的多示例学习模型。由于感兴趣的正样本视频片段只占一个视频样本的一个小部分，因此可以用一个完整的视频(video) 作为一个包 (bag) ，视频的部分片段 (clip) 作为示例(instance)，每个包根据是否含有正 (positive) 片段示例给定一个粗粒度的标签(bag label)。在最近的研究当中，使用MIL方法解决VAD问题的方法被归为WS-VAD(weakly supervised)。直接用MIL对VAD任务建模存在一些重要问题：</p><ol type="1"><li>构建出的包质量会影响到预测函数 <span class="math inline">\(f\)</span>的表现。由于迭代过程中训练集实际上是每一轮从正负包当中选出来的代表，包的数量决定了每一轮迭代的数据量大小。</li><li>与标准的MIL不同的是，<strong>VAD的instances之间是具有时序关系的</strong>，而且随着采样片段长度的变化，示例的低维度特征也在变。</li><li>可能会对MIL模型造成致命影响的是对视频异常的定义：被定义为异常的片段通常是<strong>若干个连续示例</strong>（视频片段）的组合。<strong>其中的任何一个示例</strong>对上述分类器<span class="math inline">\(f\)</span>而言都<strong>不会</strong>被训练为正标记。如果增加示例长度的采样长度，一方面会使数据集的示例量总体减少，损失细节信息；另一方面会使得模型在正向推理的过程中只能对更粗粒度的视频片段作出预测。</li></ol><p>因此，对VAD进行简单直接的MIL建模是存在重要缺陷的。从以上分析可知，VAD需要解决的问题主要是：</p><ol type="1"><li>数据集的大小（需要<strong>数据增强</strong>）</li><li>示例的<strong>语义缺失</strong>（视频片段的<strong>采样方法</strong>的合理性，要么增加片段特征的局部上下文信息——上下文信息融合也很重要）</li><li><strong>池化操作</strong>的合理性（如何选出最合理的代表，maxpooling和meanpooling不能最有代表性地挑选出示例或<strong>示例组合</strong>）</li></ol><p>针对以上问题，若干文章在不同的方向研究了提升VAD性能的方法。表格记录在下一篇文章中。</p><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr><ol><li id="fn1"><p>https://zhuanlan.zhihu.com/p/377220948<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2"><p>https://zhuanlan.zhihu.com/p/377220948<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;关于视频异常检测的核心问题&quot;&gt;关于视频异常检测的核心问题&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;在过去的实验研究当中发现视频异常检测的具体实现有很多现实难以解决的问题。但是很多文章声称的具体方法却缺乏代码实现。本文对过去的文献加以介绍并总结，并添加一些实验中发现的问题，以启发</summary>
      
    
    
    
    
  </entry>
  
</feed>
