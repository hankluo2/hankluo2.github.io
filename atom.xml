<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hank Is Lolo</title>
  
  
  <link href="http://hankislolo.com/atom.xml" rel="self"/>
  
  <link href="http://hankislolo.com/"/>
  <updated>2022-12-08T10:19:28.662Z</updated>
  <id>http://hankislolo.com/</id>
  
  <author>
    <name>Hank</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>VAD Research Review</title>
    <link href="http://hankislolo.com/2022/12/08/VAD-Research-Review/"/>
    <id>http://hankislolo.com/2022/12/08/VAD-Research-Review/</id>
    <published>2022-12-08T09:36:46.000Z</published>
    <updated>2022-12-08T10:19:28.662Z</updated>
    
    <content type="html"><![CDATA[<p>本文档记录VAD的各类解决方案，并对其方法进行归纳。</p><table><colgroup><col style="width: 17%"><col style="width: 11%"><col style="width: 17%"><col style="width: 7%"><col style="width: 4%"><col style="width: 17%"><col style="width: 5%"><col style="width: 17%"></colgroup><thead><tr class="header"><th>Title</th><th style="text-align: center;">Keyword</th><th>Idea</th><th style="text-align: center;">Pooling</th><th style="text-align: left;">Advantages</th><th>Drawbacks</th><th>Note</th><th>Comment</th></tr></thead><tbody><tr class="odd"><td><a href="https://link.springer.com/article/10.1007/s11227-021-04190-9">Attention‐basedframework for weakly supervised video anomaly detection</a></td><td style="text-align: center;">GAAM, DAAM</td><td>双任务结合（异常注意力+视频特征重构）</td><td style="text-align: center;"></td><td style="text-align: left;"></td><td></td><td>文章未开源，无代码</td><td></td></tr><tr class="even"><td><a href="https://www.aaai.org/AAAI22Papers/AAAI-6637.LiS.pdf">Self-TrainingMulti-Sequence Learning with Transformer for Weakly Supervised VideoAnomaly Detection</a></td><td style="text-align: center;">MSL</td><td>将Instance改为Sequence，在迭代优化的过程中逐渐减小Sequence的大小，使预测信号在时间域上逐渐分离。使用CTE模块，引入的Conv可以保留局部上下文特征</td><td style="text-align: center;">Sequence-Pooling (Dynamic)</td><td style="text-align: left;">自适应的渐变优化</td><td>尚未复现。初步实验发现CTE在全局的训练过程中会让所有的特征分数下降为0或者1（这种现象似乎是正常的？）过程实现很繁琐</td><td>无代码</td><td></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/abstract/document/9774889">WeaklySupervised Video Anomaly Detection via Transformer-Enabled TemporalRelation Learning</a></td><td style="text-align: center;">Transformer, Temporal RelationLearning</td><td>在I3D特征后引入上下文的Transformer自注意力模块，将上下文信息融合到每一个示例的特征（高阶）当中</td><td style="text-align: center;">mean</td><td style="text-align: left;"></td><td></td><td>未开源</td><td>上下文注意力肯定是合理的，但是它比Conv模块提取局部信息快了多少？</td></tr><tr class="even"><td></td><td style="text-align: center;"></td><td></td><td style="text-align: center;"></td><td style="text-align: left;"></td><td></td><td></td><td></td></tr><tr class="odd"><td></td><td style="text-align: center;"></td><td></td><td style="text-align: center;"></td><td style="text-align: left;"></td><td></td><td></td><td></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文档记录VAD的各类解决方案，并对其方法进行归纳。&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 17%&quot;&gt;
&lt;col style=&quot;width: 11%&quot;&gt;
&lt;col style=&quot;width: 17%&quot;&gt;
&lt;col style=&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>VAD Research Logs</title>
    <link href="http://hankislolo.com/2022/12/07/VAD-Research-Logs/"/>
    <id>http://hankislolo.com/2022/12/07/VAD-Research-Logs/</id>
    <published>2022-12-07T09:33:16.000Z</published>
    <updated>2022-12-08T09:35:34.199Z</updated>
    
    <content type="html"><![CDATA[<h3 id="关于视频异常检测的核心问题">关于视频异常检测的核心问题</h3><hr><p>在过去的实验研究当中发现视频异常检测的具体实现有很多现实难以解决的问题。但是很多文章声称的具体方法却缺乏代码实现。本文对过去的文献加以介绍并总结，并添加一些实验中发现的问题，以启发后续的工作继续深入。</p><h4 id="mil-multiple-instance-learning-的一般方法">MIL (MultipleInstance Learning) 的一般方法</h4><p>对于一个典型的MIL问题，解决这个方法通常为<strong>迭代优化（alternativeoptimization)。</strong>也就是说，我们先假设已经知道了所有样本的标记，那么就可以通过某种监督学习的方法得到一个分类模型，通过这个模型我们可以对每个训练样本进行预测，然后更新它们的标记，我们又可以拿这一次新得到的标记重新训练分类模型了。所以整个优化过程分为两部分：监督学习，标记更新<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>。值得注意的是，在训练的过程中：</p><ol type="1"><li>第一点，训练监督学习的模型的时候，只从正样本包里挑选被预测的“最像正确”(也就是分类得分最高)的那一个，正样本包里面<strong>其他的</strong>样本，不管预测出来是正的还是负的<strong>都不要了</strong>。这是因为，其实多示例的问题也可以描述为，正样本包里面“最正确”的一个样本标记是正的，跟其他样本无关。所以，这种选择策略恰恰是符合问题定义的。<strong>（选最容易分类的，因为可能只有一个）</strong></li><li>第二点，如果负样本足够多的话，可以只挑选每个负样本包里面被预测“最像正确"的一个样本作为负样本进行训练，这样子的负样本也叫做hardsample或者most violatedsample。实践上来说，它们对于模型快速收敛是最有效的。<strong>（选最难分类的，因为所有样本都是负的，并且要使模型能够有区分度，这种观点可以参考支持向量机中的支持向量的观点）</strong><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li></ol><p>因此，对于一个经典的MIL问题，通常的算法实现过程分为如下步骤：</p><p>多示例学习：</p><p>输入：正包，负包</p><p>输出： 分类函数 <span class="math inline">\(f\)</span></p><p>将每个标记包中的样本初始化为<strong>包的标记</strong>，初始化集合<span class="math inline">\(U\)</span> 为空，将所有样本 (instance)加入样本集 <span class="math inline">\(U\)</span></p><p>重复下面的过程：</p><ol type="1"><li>取 <strong><span class="math inline">\(U\)</span>中所有样本</strong> <span class="math inline">\(s_i\)</span> 以及标记<span class="math inline">\(y\)</span> <em>训练</em> 得到一个分类函数<span class="math inline">\(f\)</span>（初始状态：所有包的样本都已经加入<span class="math inline">\(U\)</span>）</li><li>利用 <span class="math inline">\(f\)</span> <em>预测</em><strong>训练集的所有样本</strong>的标记 <span class="math inline">\(\hat{y}\)</span></li><li>清空 <span class="math inline">\(U\)</span></li><li>对于每个正标记包，选取 <span class="math inline">\(f\)</span>预测得分最高的样本 <span class="math inline">\(s_{p\_max}\)</span>加入集合 <span class="math inline">\(U\)</span>（选最容易分类的一个）</li><li>对于每个负标记包，选取 <span class="math inline">\(f\)</span>预测得分最高的样本 <span class="math inline">\(s_{n\_max}\)</span>加入集合 <span class="math inline">\(U\)</span>（选最难分类的一个或多个）</li></ol><p>直到 <span class="math inline">\(U\)</span> 中不包含正样本为止.</p><p>返回 <span class="math inline">\(f\)</span>.</p><p>伪代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Input: Negative_bags (N), Positive_bags (P)</span></span><br><span class="line"><span class="string">Output: Classifier f</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Args:</span></span><br><span class="line"><span class="string">iter_num: iterations of each training stage</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># Initialize labels for each of the instances of neg/pos bags</span></span><br><span class="line"><span class="keyword">for</span> instance, label <span class="keyword">in</span> N_bags:</span><br><span class="line">label = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> instance, label <span class="keyword">in</span> P_bags:</span><br><span class="line">label = <span class="number">1</span></span><br><span class="line">U.add(N, P)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative Optimization</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> condition:</span><br><span class="line">  <span class="comment"># Train model with current pseudo-labelled U</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iter_num):</span><br><span class="line">        model.train(U)  <span class="comment"># All positive instances are assigned as 1, else 0</span></span><br><span class="line">    U.clear()</span><br><span class="line">    N_bag, P_bag = [], []</span><br><span class="line">    <span class="keyword">for</span> bag <span class="keyword">in</span> N:</span><br><span class="line">        neg_instances = model.inference(bag)</span><br><span class="line">        N_bag.append(<span class="built_in">max</span>(neg_instances))  <span class="comment"># Pooling stage, add the most likely instance (representative) into training set U.</span></span><br><span class="line">    <span class="keyword">for</span> bag <span class="keyword">in</span> P:</span><br><span class="line">        pos_instances = model.inference(bag)</span><br><span class="line">        P_bag.append(<span class="built_in">max</span>(pos_instances))  <span class="comment"># Max-pooling</span></span><br><span class="line">    U.add(N_bag, P_bag)</span><br></pre></td></tr></table></figure><p>在迭代循环的过程中，1~3为迭代推理阶段，4~5为池化阶段 (Pooling) 。</p><p><del>池化操作会丢失大量的不必要数据。以以上方法为例，池化采用top-1则每一轮迭代的样本个数随包的大小指数级下降。在池化开始前，设每个包含有的样本数量大小<span class="math inline">\(N\)</span> 相同，<span class="math inline">\(U\)</span> 中的包个数为 <span class="math inline">\(m\)</span> ，则当前参与模型训练的样本 <span class="math inline">\(I\)</span> 的个数为 <span class="math inline">\(N\times m\)</span>. 在正负包平衡的状态下，池化操作使得 <span class="math inline">\(U\)</span> 包含的样本个数下降为 <span class="math inline">\(m\)</span>. 如果不能保证 <span class="math inline">\(m &gt;&gt; N\)</span>，</del></p><p>除了初始化将所有样本全部加入<span class="math inline">\(U\)</span>的情况，每一次迭代训练的正负样本数与池化操作得到的样本总量相同（在本例里，采用最大池化每个包挑选出一个样本作为代表）。每一次迭代完的模型在推理过程中会对正负包里面的所有样本重新打上0/1标签。</p><p>从以上MIL的一般求解范式来看，MIL具有以下特点：</p><ul><li>示例（instance,以上称为样本，为方便讨论以下将其称为“示例”）具有Permutation-Invariance特性，即交换示例的位置不会对打分造成影响（打分与位置无关）</li><li>模型的训练过程中“包”的概念被解除，预测模型的输入为单个的示例（更加细的粒度）</li><li><input type="checkbox" disabled>目标预测函数最好对数据分布具有一定程度的先验知识（第一轮选出来的示例代表非常重要，如果选择错误会导致后续的训练也产生错误）</li></ul><h4 id="vad-不那么mil的mil">VAD: 不那么MIL的MIL</h4><p>VAD可以抽象为一个典型的多示例学习模型。由于感兴趣的正样本视频片段只占一个视频样本的一个小部分，因此可以用一个完整的视频(video) 作为一个包 (bag) ，视频的部分片段 (clip) 作为示例(instance)，每个包根据是否含有正 (positive) 片段示例给定一个粗粒度的标签(bag label)。在最近的研究当中，使用MIL方法解决VAD问题的方法被归为WS-VAD(weakly supervised)。直接用MIL对VAD任务建模存在一些重要问题：</p><ol type="1"><li>构建出的包质量会影响到预测函数 <span class="math inline">\(f\)</span>的表现。由于迭代过程中训练集实际上是每一轮从正负包当中选出来的代表，包的数量决定了每一轮迭代的数据量大小。</li><li>与标准的MIL不同的是，<strong>VAD的instances之间是具有时序关系的</strong>，而且随着采样片段长度的变化，示例的低维度特征也在变。</li><li>可能会对MIL模型造成致命影响的是对视频异常的定义：被定义为异常的片段通常是<strong>若干个连续示例</strong>（视频片段）的组合。<strong>其中的任何一个示例</strong>对上述分类器<span class="math inline">\(f\)</span>而言都<strong>不会</strong>被训练为正标记。如果增加示例长度的采样长度，一方面会使数据集的示例量总体减少，损失细节信息；另一方面会使得模型在正向推理的过程中只能对更粗粒度的视频片段作出预测。</li></ol><p>因此，对VAD进行简单直接的MIL建模是存在重要缺陷的。从以上分析可知，VAD需要解决的问题主要是：</p><ol type="1"><li>数据集的大小（需要<strong>数据增强</strong>）</li><li>示例的<strong>语义缺失</strong>（视频片段的<strong>采样方法</strong>的合理性，要么增加片段特征的局部上下文信息——上下文信息融合也很重要）</li><li><strong>池化操作</strong>的合理性（如何选出最合理的代表，maxpooling和meanpooling不能最有代表性地挑选出示例或<strong>示例组合</strong>）</li></ol><p>针对以上问题，若干文章在不同的方向研究了提升VAD性能的方法。表格记录在下一篇文章中。</p><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr><ol><li id="fn1"><p>https://zhuanlan.zhihu.com/p/377220948<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2"><p>https://zhuanlan.zhihu.com/p/377220948<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;关于视频异常检测的核心问题&quot;&gt;关于视频异常检测的核心问题&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;在过去的实验研究当中发现视频异常检测的具体实现有很多现实难以解决的问题。但是很多文章声称的具体方法却缺乏代码实现。本文对过去的文献加以介绍并总结，并添加一些实验中发现的问题，以启发</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Say Hi to Roxy</title>
    <link href="http://hankislolo.com/2022/12/07/Say-Hi-to-Roxy/"/>
    <id>http://hankislolo.com/2022/12/07/Say-Hi-to-Roxy/</id>
    <published>2022-12-07T05:36:06.000Z</published>
    <updated>2022-12-07T15:52:34.422Z</updated>
    
    <content type="html"><![CDATA[<p>Hello, Roxy! This is 4 U! Hank is Lolo.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hello, Roxy! This is 4 U! Hank is Lolo.&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
</feed>
